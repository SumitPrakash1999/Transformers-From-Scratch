# Transformers-From-Scratch
## Overview
This project implements a Transformer model for machine translation, using English and French as the source and target languages. The model is trained using the TED Talks dataset and SentencePiece for tokenization. Hyperparameters such as the number of layers, attention heads, embedding dimensions, and dropout are varied for optimization.

---

## Prerequisites
Before running the project, ensure you have the following dependencies installed:

- Python 3.7+
- PyTorch
- SentencePiece
- sacrebleu
- scikit-learn
- matplotlib
- tqdm (for progress bars)

To install these dependencies, run:

```bash
pip install torch sentencepiece sacrebleu scikit-learn matplotlib tqdm
```

## Instructions to Run the Model
1. Download the zip and extract the project files.

2. Prepare the Dataset
Download the TED Talks dataset (train, validation, and test splits). Ensure the following structure for your dataset:

```plaintext
/data/
    - train.en
    - train.fr
    - dev.en
    - dev.fr
    - test.en
    - test.fr
```
3. Train the SentencePiece Models
You can train the SentencePiece models using the following commands:

```bash
spm.SentencePieceTrainer.train('--input=train.en --model_prefix=eng_model --vocab_size=10000 --user_defined_symbols=<pad>')
spm.SentencePieceTrainer.train('--input=train.fr --model_prefix=fr_model --vocab_size=10000 --user_defined_symbols=<pad>')
```
4. Train the Transformer Model
To train the model, run the following script:

```bash
python train_transformer.py
```
This script trains the Transformer model and saves the best-performing model weights based on validation BLEU score.

---

## Loading the Pretrained Model
1. Download the pretrained model weights from this link.

2. Load the Pretrained Weights
Modify the script to load the pretrained model before evaluation:

```python
model.load_state_dict(torch.load('path_to_pretrained_model.pth'))
```
3. Run Evaluation
To evaluate the pretrained model on the test set, run:

```bash
python test_transformer.py
```
---
## Model Assumptions
The dataset is pre-tokenized using SentencePiece, with padding index 0 and vocabulary sizes of 10,000 for both languages.
The model architecture assumes the following default hyperparameters:
- Encoder/Decoder layers: 6
- Attention heads: 8
- Embedding dimension: 512
- Feedforward dimension: 2048
- Dropout: 0.1


## Evaluation Metrics
The model is evaluated using the BLEU score on the validation and test sets. 
For sentence-level BLEU scores, check the output file testbleu.txt generated by the evaluation script.
